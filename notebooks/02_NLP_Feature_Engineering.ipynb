{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d58d736-959c-41aa-a717-fa5e4d09d0b6",
   "metadata": {},
   "source": [
    "In this section, we are going to do feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b9b743-88e9-4740-a997-2eb4e4f8ef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded: 110840 rows\n",
      "Loading AI Model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddce2b9e7f734c75909d4c3f4bf6c2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Sentiment Analysis on sample (first 1000 rows)...\n",
      "Preview of Results:\n",
      "                              review_comment_message  review_score  \\\n",
      "0  Não testei o produto ainda, mas ele veio corre...           4.0   \n",
      "1                               Muito bom o produto.           4.0   \n",
      "2                                            neutral           5.0   \n",
      "3  O produto foi exatamente o que eu esperava e e...           5.0   \n",
      "4                                            neutral           5.0   \n",
      "5                                            neutral           4.0   \n",
      "6                                            neutral           5.0   \n",
      "7                                            neutral           1.0   \n",
      "8                                            neutral           5.0   \n",
      "9                         Aguardando retorno da loja           1.0   \n",
      "\n",
      "   sentiment_score  \n",
      "0                3  \n",
      "1                5  \n",
      "2                3  \n",
      "3                5  \n",
      "4                3  \n",
      "5                3  \n",
      "6                3  \n",
      "7                3  \n",
      "8                3  \n",
      "9                4  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "#We start with setting up paths\n",
    "PROCESSED_PATH = os.path.join(\"..\", \"data\", \"processed\")\n",
    "INPUT_FILE = os.path.join(PROCESSED_PATH, \"merged_transactions.parquet\")\n",
    "\n",
    "#We now want to load the data.\n",
    "df= pd.read_parquet(INPUT_FILE)\n",
    "print(f\"Data Loaded: {len(df)} rows\")\n",
    "\n",
    "# We now have to handle the missing reviews\n",
    "#This is the business logic, if a user did not write a review, we fill NaNs with a neutral placeholder text\n",
    "df['review_comment_message'] = df['review_comment_message'].fillna(\"neutral\")\n",
    "\n",
    "#We now initialize the AI model, we use Multilingual BERT Model fine-tuned for sentiment\n",
    "print(\"Loading AI Model...\")\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model= \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    ")\n",
    "\n",
    "#The Batch processing logic: we are processing this in chunks so that we can see a progress bar\n",
    "def get_sentiment_scores(texts):\n",
    "    \"\"\"\n",
    "    Input: List of text strings\n",
    "    Output: List of scores (1 to 5)\n",
    "    \"\"\"\n",
    "    # The model returns labels like '5 stars', '4 stars'. We want just the integer.\n",
    "    results = sentiment_pipeline(texts, truncation=True, max_length=512)\n",
    "    return [int(res['label'].split()[0]) for res in results]\n",
    "\n",
    "# WARNING: This step can take time (30-60 mins on CPU). \n",
    "# For testing now, let's run it on the first 1,000 rows to make sure it works.\n",
    "# Once it works, you can remove the [:1000] slice to run on all data.\n",
    "\n",
    "print(\"Starting Sentiment Analysis on sample (first 1000 rows)...\")\n",
    "sample_df = df.head(1000).copy() # <--- CHANGE THIS later to run on full data\n",
    "\n",
    "# We convert the text column to a list\n",
    "texts = sample_df['review_comment_message'].tolist()\n",
    "\n",
    "# Run the model\n",
    "scores = get_sentiment_scores(texts)\n",
    "\n",
    "# Save the scores back to the dataframe\n",
    "sample_df['sentiment_score'] = scores\n",
    "\n",
    "print(\"Preview of Results:\")\n",
    "print(sample_df[['review_comment_message', 'review_score', 'sentiment_score']].head(10))\n",
    "\n",
    "# 6. Save (Partial)\n",
    "# sample_df.to_parquet(os.path.join(PROCESSED_PATH, \"transactions_with_sentiment.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996b8092-8281-4a28-aac1-b376eaef7fd2",
   "metadata": {},
   "source": [
    "since we ran the above code on 1000 rows, we now need to run it on the entire dataset of 110k rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88455fc7-475a-42d0-a773-3d035f548887",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Full Production on ALL rows (This may take a while)...\n",
      "Starting batch processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██| 3464/3464 [3:39:29<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE! Processed data saved to: ..\\data\\processed\\transactions_with_sentiment.parquet\n",
      "   review_score  sentiment_score\n",
      "0           4.0                3\n",
      "1           4.0                5\n",
      "2           5.0                3\n",
      "3           5.0                5\n",
      "4           5.0                3\n"
     ]
    }
   ],
   "source": [
    "# --- PRODUCTION RUN ---\n",
    "\n",
    "# 1. Choose your size\n",
    "# Set this to None to run ALL data (Best for final project)\n",
    "# Set to 20000 if you want to finish in ~15 mins for testing\n",
    "SAMPLE_SIZE = None \n",
    "\n",
    "if SAMPLE_SIZE:\n",
    "    print(f\"Running Fast Track on {SAMPLE_SIZE} rows...\")\n",
    "    df_to_process = df.head(SAMPLE_SIZE).copy()\n",
    "else:\n",
    "    print(\"Running Full Production on ALL rows (This may take a while)...\")\n",
    "    df_to_process = df.copy()\n",
    "\n",
    "# 2. Optimized Processing Function\n",
    "def get_sentiment_batches(text_list, batch_size=32):\n",
    "    # We send text to the model in groups of 32 to speed up CPU processing\n",
    "    results = []\n",
    "    # tqdm makes the progress bar look nice\n",
    "    for i in tqdm(range(0, len(text_list), batch_size)):\n",
    "        batch = text_list[i : i + batch_size]\n",
    "        predictions = sentiment_pipeline(batch, truncation=True, max_length=512)\n",
    "        # Extract just the star rating (integer)\n",
    "        scores = [int(p['label'].split()[0]) for p in predictions]\n",
    "        results.extend(scores)\n",
    "    return results\n",
    "\n",
    "# 3. Run It\n",
    "texts = df_to_process['review_comment_message'].tolist()\n",
    "print(\"Starting batch processing...\")\n",
    "\n",
    "# This is where the magic happens\n",
    "df_to_process['sentiment_score'] = get_sentiment_batches(texts)\n",
    "\n",
    "# 4. Save Final Output\n",
    "output_file = os.path.join(PROCESSED_PATH, \"transactions_with_sentiment.parquet\")\n",
    "df_to_process.to_parquet(output_file, index=False)\n",
    "\n",
    "print(f\"DONE! Processed data saved to: {output_file}\")\n",
    "print(df_to_process[['review_score', 'sentiment_score']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ab15d9-4b8e-4587-b567-0a7fead8b352",
   "metadata": {},
   "source": [
    "We now want to do Transformation or aggregation. We will do a time series and aggregate this data by day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d00b9c1-6dc1-4432-908c-bc3f74dc764e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
