{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d58d736-959c-41aa-a717-fa5e4d09d0b6",
   "metadata": {},
   "source": [
    "In this section, we are going to do feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b9b743-88e9-4740-a997-2eb4e4f8ef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded: 110840 rows\n",
      "Loading AI Model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7414dbbfe27048e79d84b3c6bcc093c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b675e6ce24e4a30a932defb49cb6c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120842d29ef44cf082ab4ce886ca72eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f4bba2e37784a17aa5072642ce83b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0275c457073846c5ba248b541f6f3bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582e8d3e40a14aa097d24a19895d151d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Sentiment Analysis on sample (first 1000 rows)...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "#We start with setting up paths\n",
    "PROCESSED_PATH = os.path.join(\"..\", \"data\", \"processed\")\n",
    "INPUT_FILE = os.path.join(PROCESSED_PATH, \"merged_transactions.parquet\")\n",
    "\n",
    "#We now want to load the data.\n",
    "df= pd.read_parquet(INPUT_FILE)\n",
    "print(f\"Data Loaded: {len(df)} rows\")\n",
    "\n",
    "# We now have to handle the missing reviews\n",
    "#This is the business logic, if a user did not write a review, we fill NaNs with a neutral placeholder text\n",
    "df['review_comment_message'] = df['review_comment_message'].fillna(\"neutral\")\n",
    "\n",
    "#We now initialize the AI model, we use Multilingual BERT Model fine-tuned for sentiment\n",
    "print(\"Loading AI Model...\")\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model= \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    ")\n",
    "\n",
    "#The Batch processing logic: we are processing this in chunks so that we can see a progress bar\n",
    "def get_sentiment_scores(texts):\n",
    "    \"\"\"\n",
    "    Input: List of text strings\n",
    "    Output: List of scores (1 to 5)\n",
    "    \"\"\"\n",
    "    # The model returns labels like '5 stars', '4 stars'. We want just the integer.\n",
    "    results = sentiment_pipeline(texts, truncation=True, max_length=512)\n",
    "    return [int(res['label'].split()[0]) for res in results]\n",
    "\n",
    "# WARNING: This step can take time (30-60 mins on CPU). \n",
    "# For testing now, let's run it on the first 1,000 rows to make sure it works.\n",
    "# Once it works, you can remove the [:1000] slice to run on all data.\n",
    "\n",
    "print(\"Starting Sentiment Analysis on sample (first 1000 rows)...\")\n",
    "sample_df = df.head(1000).copy() # <--- CHANGE THIS later to run on full data\n",
    "\n",
    "# We convert the text column to a list\n",
    "texts = sample_df['review_comment_message'].tolist()\n",
    "\n",
    "# Run the model\n",
    "scores = get_sentiment_scores(texts)\n",
    "\n",
    "# Save the scores back to the dataframe\n",
    "sample_df['sentiment_score'] = scores\n",
    "\n",
    "print(\"Preview of Results:\")\n",
    "print(sample_df[['review_comment_message', 'review_score', 'sentiment_score']].head(10))\n",
    "\n",
    "# 6. Save (Partial)\n",
    "# sample_df.to_parquet(os.path.join(PROCESSED_PATH, \"transactions_with_sentiment.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee5e31e-57af-445d-bd23-aad5e6906323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
